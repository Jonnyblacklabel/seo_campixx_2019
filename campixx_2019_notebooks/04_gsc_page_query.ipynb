{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSC - Suchanfragen & Seiten\n",
    "Bisher haben wir uns rein auf die Gesamtwerte über Zeit beschränkt. Natürlich bieten die Suchanalyse-Daten ebenfalls Suchphrasen und die dazu angezeigten Zielseiten. Für uns SEOs gibt es hierin viele nützliche Informationen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run helpers/code_toggle.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import register_matplotlib_converters\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -r 33:38 helpers/helpers.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden der Daten aus unserer Datenbank in einen DataFrame\n",
    "Wie schon zuvor, laden wir die Daten, die wir ja nun glücklicherweise in der sqlite Datenbank vorliegen haben mit Pandas direkt in einen DataFrame. Dabei wollen wir nun nicht wie zuvor mittels `pd.read_sql_table` die gesamte Tabelle laden. In diesem Beispiel Fragen wir die Daten mit `pd.read_sql` [(link)](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html#pandas.read_sql) für einen definierten SQL-Query ab. Dabei holen wir die Daten für web, country, device, page & query, die in der entsprechenden Tabelle liegen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = dataset.connect('sqlite:///data/serienjunkies.db')\n",
    "\n",
    "query = \"\"\"\n",
    "select \"date\",\n",
    "    page,\n",
    "    query,\n",
    "    sum(clicks) as clicks,\n",
    "    sum(impressions) as impressions,\n",
    "    sum(impressions * position) as pos_imp\n",
    "from web_country_device_page_query\n",
    "where \"date\" > '2019-01-01'\n",
    "group by \"date\", page, query\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(query, con=db.engine, parse_dates=['date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentieren nach Suchphrasen und Seiten\n",
    "Da die GSC Daten sehr umfangreich sind, ist es sehr sinnvoll die Daten zu segmentieren. Dabei schaffen wir weitere Möglichkeiten, den Datenwust zu analysieren.\n",
    "Dabei werden wir die Suchphrasen in Brand und NonBrand und die Seiten in verschiedene Seitensegmente einteilen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -r 44:105 helpers/helpers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = {\n",
    "    \"brand_patterns\": ['serienjunk'],\n",
    "    \"page_patterns\" : {\n",
    "        \"serie\" : {\n",
    "            \"Serie XYZ\" : [\"^https:\\/\\/www\\.serienjunkies\\.de\\/[^\\/]+\\/$\"],\n",
    "            \"Serie XYZ Ausstrahlungstermine TV\" : [\"^https:\\/\\/www\\.serienjunkies\\.de\\/[^\\/]+\\/tv\\/\"],\n",
    "            \"Serie XYZ News\" : [\"^https:\\/\\/www\\.serienjunkies\\.de\\/[^\\/]+\\/news\\/\"],\n",
    "            \"Serie XYZ Streams\" : [\"^https:\\/\\/www\\.serienjunkies\\.de\\/[^\\/]+\\/stream\\/\"],\n",
    "            \"Serie XYZ Poster Übersicht\" : [\"^https:\\/\\/www\\.serienjunkies\\.de\\/[^\\/]+\\/poster\\/$\"],\n",
    "            \"Serie XYZ Posterseite\" : [\"^https:\\/\\/www\\.serienjunkies\\.de\\/[^\\/]+\\/poster\\/.+\"],\n",
    "            \"Serie XYZ Episodenguides\" : [\"^https:\\/\\/www\\.serienjunkies\\.de\\/[^\\/]+\\/alle-serien-staffeln\\.html\"],\n",
    "            \"Serie XYZ Staffel N\" : [\"^https:\\/\\/www\\.serienjunkies\\.de\\/[^\\/]+\\/season\\d+\\.html\"],\n",
    "            \"Serie XYZ Reviewsübersicht\" : [\"^https:\\/\\/www\\.serienjunkies\\.de\\/[^\\/]+\\/reviews\\/$\"],\n",
    "            \"Serie XYZ Review\" : [\"^https:\\/\\/www\\.serienjunkies\\.de\\/[^\\/]+\\/reviews\\/\\d{1,3}x\\d{1,3}.+\\.html\"],\n",
    "            \"Serie XYZ Episodenseite\" : [\"^https:\\/\\/www\\.serienjunkies\\.de\\/[^\\/]+\\/\\d{1,3}x\\d{1,3}.+\\.html\"]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.pipe(segment_brand, segments) \\\n",
    "    .pipe(segment_pages, segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('serie', as_index=False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einfacher Plot der Segmente\n",
    "Nun da wir die Seiten in Segmente eingeteilt haben, ist es ein leichtes die Leistungsdaten der GSC für diese Segmente darzustellen. Zum einen natürlich als Balkendiagramm, um die Verhältnisse sehen zu können. Zum anderen aber auch Zeitreihen, da wir die Daten inkl. \"date\" Spalte im DataFrame haben. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.groupby('serie', as_index=False)['clicks'].sum() \\\n",
    "    .sort_values('clicks', ascending=False) \\\n",
    "    .pipe((sns.barplot, 'data'),\n",
    "          x='clicks', y='serie',\n",
    "          orient='h')\n",
    "ax.set_title('Serien-Segmente nach Klicks', x=0, ha='left')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.constrained_layout.use':False}) # constained layout wird von seaborn aktuell für Grids nicht unterstützt.\n",
    "df.groupby(['serie','brand_nonbrand'], as_index=False)['clicks'].sum() \\\n",
    "    .sort_values('clicks', ascending=False) \\\n",
    "    .pipe((sns.catplot, 'data'),\n",
    "          x='clicks', y='serie', col='brand_nonbrand',\n",
    "          orient='h', kind='bar', aspect=1.5)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 33:38 helpers/helpers.py\n",
    "sns.set(context='talk',\n",
    "        rc={'figure.figsize':(15,6),\n",
    "            'axes.titlepad':18,\n",
    "            'axes.titlesize':22,\n",
    "            'figure.constrained_layout.use':True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmente als Stripplot\n",
    "Neben dem gut bekannten Barchart, versuchen wir nun einen etwas cooleren Plot. Im Seaborn-Package, `stripplot` [(link)](https://seaborn.pydata.org/generated/seaborn.stripplot.html#seaborn.stripplot) genannt, zeigt es jede einzelne Suchanfrage (oder Seite), ähnlich wie in einem Scatterplot. In diesem Beispiel zeigen wir zusätzlich die Aufteilung in Brand und Nonbrand. Hiermit erhalten wir einen sehr guten Eindruck davon, welche Bereiche herausstechen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.query('serie != \"unknown\"') \\\n",
    "    .groupby(['serie','brand_nonbrand', 'page'], as_index=False).sum() \\\n",
    "    .sort_values('clicks', ascending=False) \\\n",
    "    .pipe((sns.stripplot, 'data'), x='clicks', y='serie',\n",
    "          hue='brand_nonbrand', alpha=.5)\n",
    "ax.set_title('Seiten pro Segment nach Klicks', x=0, ha='left')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natürlich können wir uns die entsprechenden Seiten mit den Maximalwerten auch ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('serie != \"unknown\"') \\\n",
    "    .groupby(['serie','brand_nonbrand', 'page'], as_index=False)['clicks'].sum() \\\n",
    "    .groupby(['serie', 'brand_nonbrand']).max().unstack(level=1) \\\n",
    "    .sort_values([('clicks','nonbrand')], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.query('serie != \"unknown\"') \\\n",
    "    .groupby(['serie','brand_nonbrand', 'query'], as_index=False).sum() \\\n",
    "    .sort_values('clicks', ascending=False) \\\n",
    "    .pipe((sns.stripplot, 'data'), x='clicks', y='serie',\n",
    "          hue='brand_nonbrand', alpha=.5)\n",
    "ax.set_title('Suchphrasen pro Segment nach Klicks', x=0, ha='left')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch hier eine Ausgabe der Top Queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('serie != \"unknown\"') \\\n",
    "    .groupby(['serie','brand_nonbrand', 'query'], as_index=False)['clicks'].sum() \\\n",
    "    .groupby(['serie', 'brand_nonbrand']).max().unstack(level=1) \\\n",
    "    .sort_values([('clicks','nonbrand')], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Terme eines Segments\n",
    "Zur Optimierung von Seiten, oder zum \"kennenlernen\" eines Seitensegments, kann man sich sehr gut die Einzelterme des Datensatzes ansehen. So bekommt man einen recht guten Eindruck davon, welche Einzel-Worte häufig gesucht werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('serie == \"Serie XYZ Episodenguides\"') \\\n",
    "    .groupby(['query'], as_index=False).sum() \\\n",
    "    ['query'].str.split(expand=True) \\\n",
    "    .melt() \\\n",
    "    .groupby('value').count() \\\n",
    "    .sort_values('variable', ascending=False) \\\n",
    "    .head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spaß mit fuzzy wuzzy\n",
    "Pattern Matching per Distanz-Funktion. Im obrigen Beispiel haben wir die Segmente \"nur\" auf Basis von Regex-Pattern gebildet. Natürlich gibt es noch wesentlich coolere Methoden. Zum Beispiel können per Levenshtein Distanz ähnliche Suchphrasen gefunden werden. Das Package gibt uns Keywords aus unserem Datenset die eine definierte Ähnlichkeit zum gesuchten Wort aufweisen. \n",
    "\n",
    "*Achtung: Wenn du das Package `python-Levenshtein` nicht installieren konntest, gibt es eine Warnung und alles läuft etwas langsamer.*\n",
    "\n",
    "Das Package führt dabei ein simples Preprocessing durch (lowercase, sorting, etc). Genauere Infos gibt es bei Datacamp https://www.datacamp.com/community/tutorials/fuzzy-string-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.token_sort_ratio('the walking dead', 'are the dead walking?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klicks mit ähnlicher Suchphrase\n",
    "Im Beispiel wollen wir nun nur Klicks visualisieren, die durch Suchphrasen generiert wurden, die sehr ähnlich zu unserem gesuchten Begriff sind. Wir suchen im Beispiel nach `the walking dead`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_queries = df['query'].unique()\n",
    "len(unique_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_score = 90\n",
    "\n",
    "possibilities = process.extract(\"the walking dead\", unique_queries, limit=100, scorer=fuzz.token_sort_ratio)\n",
    "\n",
    "hits = pd.DataFrame([possible for possible in possibilities if possible[1] > min_score], columns=['query', 'score']) \\\n",
    "    .set_index('query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun joinen wir einfach den DataFrame mit den Suchphrasen die uns \"ähnlich\" genug sind, an unsere Daten aus der Datenbank. Wir führen dabei einen `right-join` durch. So erhalten wir nur die Zeilen die Suchphrasen aus unserer Treffermenge erhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.groupby(['date','query']).sum() \\\n",
    "    .join(hits, on='query', how='right') \\\n",
    "    .groupby(level=0).sum() \\\n",
    "    .plot(y='clicks')\n",
    "ax.set_title('Klicks für \"The Walking Dead\"', x=0, ha='left')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Top Performer & Low hanging fruits\n",
    "Als letztes Beispiel machen wir eine klassische Analyse. Wir suchen uns Seiten aus dem Segment `Serie XYZ`, die - bezogen auf CTR vs. Position - schlechter als der Durchschnitt in diesem Seitensegment performen. Damit wir etwas mehr sehen, beschränken wir uns auf Seiten die weniger als 100% CTR haben und eine Position besser als 21 aufweisen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorbereiten der Daten\n",
    "Wir führen nun folgende Schritte aus:\n",
    "- Filter auf 'nonbrand'\n",
    "- Filter auf Segment 'Serie XYZ'\n",
    "- Gruppieren auf Seiten\n",
    "- Berechnen von CTR und gewichteter Position\n",
    "- Berechnen von gerundeter Position\n",
    "- Filter auf Position <= 20 und CTR < 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -r 4:28 helpers/helpers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonbrand_mask = df['brand_nonbrand'].isin(['nonbrand'])\n",
    "only_serie = df['serie'].isin(['Serie XYZ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[nonbrand_mask & only_serie] \\\n",
    "    .groupby('page').sum() \\\n",
    "    .pipe(assign_position) \\\n",
    "    .pipe(assign_ctr) \\\n",
    "    .assign(pos_round = lambda x: round(x['position'], 0)) \\\n",
    "    .query('pos_round <= 20 and ctr < 1')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einfacher Stripplot\n",
    "Zunächst erstellen wir einen einfachen Stripplot. Gerundete Position vs. CTR. Jeder Punkt ist ein Keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.stripplot(x='pos_round', y='ctr', data=df2, alpha=.8)\n",
    "ax.set_title('Verteilung der CTR pro Position', x=0, ha='left')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ausreißer erkennen\n",
    "Um nun zu erkennen, welche Keywords besonders (bezogen auf Position und CTR) gut, bzw. weniger gut performen, benötigen wir eine Größe die wir zugrunde legen können. Klassischerweise nutzen wir hierfür den Wert `1.5 x IQR` ([iqr wikipedia](https://en.wikipedia.org/wiki/Interquartile_range)). Beziehungsweise `q1 - 1.5 x IQR` für negative Outlier und `q3 + 1.5 x IQR` für positive Outlier. In unserem Beispiel erachten wir alle Keywords als interessant, die sich unterhalb von Q1 befinden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x='pos_round', y='ctr', data=df2)\n",
    "# sns.stripplot(x='pos_round', y='ctr', data=df2, ax=ax, alpha=.4, color='.4', size=5, jitter=True)\n",
    "ax.set_title('Boxplot für CTR vs. Position', x=0, ha='left')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile und IQR berechnen\n",
    "Um den Datensatz filtern zu können, errechnen wir uns die Quantile, bzw. den IQR pro Positions-Gruppe. So können wir die Seiten klassifizieren und die Daten für weitere Arbeit exportieren, oder hübsch visualisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df2.groupby('pos_round')\n",
    "q1 = groups['ctr'].transform(lambda x: x.quantile(.25))\n",
    "q3 = groups['ctr'].transform(lambda x: x.quantile(.75))\n",
    "iqr = q3 - q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['performance'] = 'normal'\n",
    "df2.loc[df2['ctr'] < q1, 'performance'] = 'low'\n",
    "df2.loc[df2['ctr'] < q1 - (1.5 * iqr), 'performance'] = 'very-low'\n",
    "df2.loc[df2['ctr'] > q3, 'performance'] = 'high'\n",
    "df2.loc[df2['ctr'] > q3 + (1.5 * iqr), 'performance'] = 'very-high'\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.stripplot(x='pos_round', y='ctr', data=df2.query('pos_round <= 10'), alpha=.4, size=5, jitter=True,\n",
    "                   hue='performance', palette={'very-low':'red',\n",
    "                                               'low':'salmon',\n",
    "                                               'normal':'midnightblue',\n",
    "                                               'high':'lightgreen',\n",
    "                                               'very-high':'green'})\n",
    "ax.set_title('Performance im Segment \"Seite XYZ\"', x=0, ha='left')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-Performer exportieren\n",
    "Um die Daten zu exportieren, können wir einfach auf die entsprechende Gruppe filtern und mit `.to_csv` zum Beispiel als CSV exportieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.query('performance == \"low\"').sort_values('clicks', ascending=False).head()\n",
    "# df2.query('performance == \"low\"').sort_values('clicks', ascending=False).to_csv('low-performer.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und für alle die sich fragen wie es aussieht, wenn man die Position nicht rundet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df[nonbrand_mask & only_serie] \\\n",
    "    .groupby('page').sum() \\\n",
    "    .pipe(assign_position) \\\n",
    "    .assign(position = lambda x: round(x.position, 1)) \\\n",
    "    .pipe(assign_ctr) \\\n",
    "    .query('position <= 10 and ctr < 1')\n",
    "\n",
    "groups = df3.groupby('position')\n",
    "q1 = groups['ctr'].transform(lambda x: x.quantile(.25))\n",
    "q3 = groups['ctr'].transform(lambda x: x.quantile(.75))\n",
    "iqr = q3 - q1\n",
    "\n",
    "df3['performance'] = 'normal'\n",
    "df3.loc[df3['ctr'] < q1, 'performance'] = 'low'\n",
    "df3.loc[df3['ctr'] < q1 - (1.5 * iqr), 'performance'] = 'very-low'\n",
    "df3.loc[df3['ctr'] > q3, 'performance'] = 'high'\n",
    "df3.loc[df3['ctr'] > q3 + (1.5 * iqr), 'performance'] = 'very-high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x='position', y='ctr', data=df3, alpha=.4,\n",
    "                   hue='performance', palette={'very-low':'red',\n",
    "                                               'low':'salmon',\n",
    "                                               'normal':'midnightblue',\n",
    "                                               'high':'lightgreen',\n",
    "                                               'very-high':'green'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
